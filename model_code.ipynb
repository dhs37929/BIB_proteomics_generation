{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8959a-a9fc-4d2d-9ba1-e94b3b6ff697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import femr\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "import pickle\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import zscore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51421a-f814-4633-be7a-ad51e150f93f",
   "metadata": {},
   "source": [
    "# Create Foundation Model Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e10e6c-f8b6-4e96-b770-89eedcc6d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import femr\n",
    "import femr.etl_pipelines.simple\n",
    "import shutil\n",
    "\n",
    "INPUT_DIR = 'input/omop'\n",
    "TARGET_DIR = 'trash/extracts'\n",
    "LOG_DIR = os.path.join(TARGET_DIR, \"logs\")\n",
    "EXTRACT_DIR = os.path.join(TARGET_DIR, \"extract\")\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)\n",
    "\n",
    "# Run extract generation\n",
    "os.system(f\"etl_generic_omop {INPUT_DIR} {EXTRACT_DIR} {LOG_DIR} --num_threads 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69f297-6f38-42b2-9034-6537683f6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels (a dataframe in specified format of FEMRv1)\n",
    "## for every patient, take the DOS (days to onset of labor) number and subtract it from corresponding child_birth_date of liked EHR\n",
    "labeldf['child_birth_date'] = pd.to_datetime(labeldf['child_birth_date'])\n",
    "labeldf['protein_date'] = labeldf['child_birth_date'] + pd.to_timedelta(labeldf['DOS'], unit='D')\n",
    "labeldf['protein_date'] = pd.to_datetime(labeldf['protein_date']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "labeldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f4902-5727-4c2e-8351-4e8a9e11674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL = '.path_to_foundation_model/'\n",
    "EXTRACT_LOCATION = './trash/extracts/extract'\n",
    "LABELS = './trash/labels/labeldf.csv'\n",
    "\n",
    "# Compute Foundation Model Representation\n",
    "os.system(\n",
    "    f\"femr_compute_representations --data_path {EXTRACT_LOCATION} --model_path {MODEL} --prediction_times_path {LABELS} --batch_size 1024 ./trash/motor_rep/motor_reprs.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a1839-1239-4cc5-bbaf-1998474b53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Open the resulting representations and take a look at the data matrix.\n",
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "REPRESENTATIONS = './trash/motor_rep/motor_reprs.pkl'\n",
    "\n",
    "with open(REPRESENTATIONS, \"rb\") as f:\n",
    "    reprs = pickle.load(f)\n",
    "\n",
    "    print(reprs.keys())\n",
    "\n",
    "    print(\"Pulling the data for the first label\")\n",
    "    print(\"Patient id\", reprs[\"patient_ids\"][1])\n",
    "    print(\"Label time\", reprs[\"prediction_times\"][1])\n",
    "    print(\"Representation\", reprs[\"representations\"][1, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240211e-4441-42c9-836c-614282d7f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert representations to dataframe for easier manipulation\n",
    "df_initial = pd.DataFrame({\n",
    "    'patient_ids': reprs['patient_ids'],\n",
    "    'labeling_time': reprs['prediction_times']\n",
    "})\n",
    "\n",
    "# Convert 'data_matrix' into a DataFrame\n",
    "df_data_matrix = pd.DataFrame(reprs['representations'], columns=[f'data_{i}' for i in range(769)])\n",
    "\n",
    "# Concatenate the two DataFrames along the columns axis\n",
    "df_reprs = pd.concat([df_initial, df_data_matrix], axis=1)\n",
    "\n",
    "# Column data_768 marks boolean labels (not used here) so drop\n",
    "df_reprs = df_reprs.drop(columns='data_768')\n",
    "df_reprs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87a0de-b5ab-4293-bc4b-34475ae9017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataframe to specific format for easier downstream processing\n",
    "# Convert 'protein_date' to the same format as 'labeling_time'\n",
    "labeldf['protein_date'] = pd.to_datetime(labeldf['protein_date']).dt.strftime('%Y-%m-%d')\n",
    "# Convert both 'labeling_time' and 'protein_date' to datetime format\n",
    "df_reprs['labeling_time'] = pd.to_datetime(df_reprs['labeling_time'])\n",
    "labeldf['protein_date'] = pd.to_datetime(labeldf['protein_date'])\n",
    "# Convert 'cln_sample_ID' to the same data type as 'patient_ids'\n",
    "labeldf['cln_sample_ID'] = labeldf['cln_sample_ID'].astype(str)\n",
    "df_reprs['patient_ids'] = df_reprs['patient_ids'].astype(str)\n",
    "\n",
    "# Merge the dataframes again to get the corresponding sample_ID\n",
    "df_merged = pd.merge(df_reprs, labeldf, left_on=['patient_ids', 'labeling_time'], right_on=['cln_sample_ID', 'protein_date'], how='left')\n",
    "\n",
    "# Keep all columns from df_reprs and just add the sample_ID column\n",
    "df_reprs_final = df_merged[['sample_ID'] + df_reprs.columns.tolist()]\n",
    "\n",
    "df_reprs_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc05b9-c9cf-4e4c-8a37-1f3e40f8efe2",
   "metadata": {},
   "source": [
    "# Training pipeline for proteomics generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff93c0-e4de-478d-9939-1c85217e5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set empty dataframe to collect stats\n",
    "data = {\"protein\": [], \"spearman_correlation\": [], \"spearman_pvalue\": [], \"best_train_rmse\": [],  \"best_val_rmse\": []}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406a70f-677c-4732-a2a1-b1b7b33b52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up empty dataframe to collect prediction values\n",
    "predvalues_df = pd.DataFrame({'sample_index': range(171)})\n",
    "# Function to populate the dataframe with new columns\n",
    "def populate_dataframe(df, poi, predictions):\n",
    "    column_name = poi + \"_protein\"\n",
    "    df[column_name] = predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b4064-6b15-4043-919a-9f790f51e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read representation and proteomics data\n",
    "df_reprs_final= pd.read_csv('.path_to_FM_representation/motor_pregnancy_cohort_reprs.csv')\n",
    "prot = pd.read_csv('.path_to_proteomics_data/proteomics_outcomes.csv')\n",
    "\n",
    "def data_org(protein_name, latentdf, prot):\n",
    "\n",
    "    # Protein of interest\n",
    "    poi = protein_name\n",
    "    poiname = poi + '_protein'\n",
    "    print(poiname)\n",
    "    \n",
    "    # Subset protein expression for poi\n",
    "    poidf = prot[[poiname, 'sample_ID']]\n",
    "    # Merge dfs and clean for xgboost by removing sample_ID and extraneous columns\n",
    "    merged_df = pd.merge(latentdf, poidf, on='sample_ID')\n",
    "    cols_remove = ['sample_ID', 'labeling_time'] ## for bypatient split\n",
    "    merged_clean = merged_df.drop(columns=cols_remove)\n",
    "\n",
    "    # Create x and y dataframes\n",
    "    X, y = merged_clean.drop(poiname, axis=1), merged_clean[[poiname, 'patient_ids']]\n",
    "    return poi, X, y\n",
    "\n",
    "\n",
    "def bootstrap_nn(poi, X, y, bootstrap):    \n",
    "    # Create data structures below for boostrapping\n",
    "    train_error_dict = {}\n",
    "    val_error_dict = {}\n",
    "    prediction_dict = {str(i): None for i in range(y.shape[0])}\n",
    "    poiname = poi + '_protein'\n",
    "    \n",
    "    unique_patient_ids = X['patient_ids'].unique()## for bypatient split\n",
    "    unique_patient_ids_df = pd.DataFrame(unique_patient_ids, columns=['patient_ids']) ## for bypatient split\n",
    "    \n",
    "    for i in range(bootstrap):\n",
    "        # Split patient IDs\n",
    "        unique_patient_ids = X['patient_ids'].unique()\n",
    "        unique_patient_ids_df = pd.DataFrame(unique_patient_ids, columns=['patient_ids'])\n",
    "        \n",
    "        # Assuming one column ('patient_ids') is not a feature\n",
    "        input_size = X.shape[1] - 1\n",
    "        # Split data 50/50 into train/val_and_test\n",
    "        train_ids_initial, test_ids = train_test_split(unique_patient_ids_df, test_size=0.5, random_state=i)\n",
    "        \n",
    "        # Split data into initial train and test sets\n",
    "        cols_remove = ['patient_ids']\n",
    "        X_train_initial = X[X['patient_ids'].isin(train_ids_initial['patient_ids'])]\n",
    "        X_test = X[X['patient_ids'].isin(test_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        y_train_initial = y[y['patient_ids'].isin(train_ids_initial['patient_ids'])]\n",
    "        y_test = y[y['patient_ids'].isin(test_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        \n",
    "        # Further split the initial training data into training and validation sets (80/20 split)\n",
    "        train_ids, val_ids = train_test_split(train_ids_initial, test_size=0.2, random_state=i)\n",
    "        \n",
    "        X_train = X_train_initial[X_train_initial['patient_ids'].isin(train_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        X_val = X_train_initial[X_train_initial['patient_ids'].isin(val_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        y_train = y_train_initial[y_train_initial['patient_ids'].isin(train_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        y_val = y_train_initial[y_train_initial['patient_ids'].isin(val_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Build Model\n",
    "        class RegressionModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(RegressionModel, self).__init__()\n",
    "                self.layer1 = nn.Linear(input_size, 32)  # Input to hidden layer\n",
    "                self.relu = nn.ReLU()\n",
    "                self.layer2 = nn.Linear(32, 1)  # Hidden to output layer\n",
    "        \n",
    "            def forward(self, x):\n",
    "                x = self.relu(self.layer1(x))\n",
    "                x = self.layer2(x)\n",
    "                return x\n",
    "        \n",
    "        model = RegressionModel()\n",
    "        \n",
    "        # Criterion and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        num_epochs = 1000\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "            val_loss = val_loss / len(val_loader.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Update best model if current validation loss is lower\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                best_epoch = epoch + 1\n",
    "        \n",
    "            # Print training and validation loss for each epoch\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Store training and validation losses for plotting\n",
    "        train_error_dict[i] = train_losses\n",
    "        val_error_dict[i] = val_losses\n",
    "        \n",
    "        # Print best epoch\n",
    "        print(f'The best model was from epoch {best_epoch} with Validation Loss: {best_val_loss:.4f}')\n",
    "        \n",
    "        # Load best model state\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Collect actual and predicted values for the best model\n",
    "        actual = []\n",
    "        predicted = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                actual.extend(labels.numpy().flatten())\n",
    "                predicted.extend(outputs.numpy().flatten())\n",
    "\n",
    "\n",
    "        # Append predictions to prediction dictionary\n",
    "        for p in range(y_test.shape[0]):\n",
    "            key_to_append = str(y_test.index[p])\n",
    "            value_to_append = predicted[p]\n",
    "            if key_to_append in prediction_dict:\n",
    "                if prediction_dict[key_to_append] is None:\n",
    "                    prediction_dict[key_to_append] = []\n",
    "                prediction_dict[key_to_append].append(value_to_append)\n",
    "                \n",
    "\n",
    "    return prediction_dict, train_error_dict, val_error_dict\n",
    "\n",
    "\n",
    "\n",
    "def plot_correlation(poi, y, prediction_dict, savepath):\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    # Calculate average prediction for each sample\n",
    "    y_avg_pred = [sum(values) / len(values) if values is not None else None for values in prediction_dict.values()]\n",
    "    \n",
    "    # Convert y_test DataFrame to a numpy array and reshape\n",
    "    cols_remove = ['patient_ids']## for bypatient split\n",
    "    y = y.drop(columns=cols_remove) ## for bypatient split\n",
    "    \n",
    "    y_np = y.to_numpy().reshape(-1)\n",
    "\n",
    "    # Calculate correlation between predicted and actual values\n",
    "    scorrelation, spvalue = spearmanr(y_np, y_avg_pred)\n",
    "    \n",
    "    # Plot predicted vs. actual values\n",
    "    plt.scatter(y_np, y_avg_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Average Predicted Values\")\n",
    "    plt.title(f\"{poi} Predicted vs. Actual\\n(Spearman Correlation: {scorrelation:.2f}, Pvalue: {spvalue})\")\n",
    "    \n",
    "    min_val = min(min(y_np), min(y_avg_pred))\n",
    "    max_val = max(max(y_np), max(y_avg_pred))\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    # Drawing a 45-degree line\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n",
    "    # Save the plot as an image with the file name \"poi.png\"\n",
    "    save_directory = savepath+\"/correlation/\"\n",
    "    modpoi = poi.replace(\"/\", \"_\")\n",
    "    plot_filename = os.path.join(save_directory, f\"{modpoi}_scorr.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "\n",
    "    plt.clf()\n",
    "    return poi, scorrelation, spvalue, y_avg_pred\n",
    "\n",
    "\n",
    "def plot_learning_curve(poi, train_error_dict, val_error_dict, savepath):\n",
    "    # Calculate the average training error per epoch across all bootstrap iterations\n",
    "    # Convert the dictionary values to a numpy array for easier calculation\n",
    "    train_errors = np.array(list(train_error_dict.values()))\n",
    "    val_errors = np.array(list(val_error_dict.values()))\n",
    "\n",
    "    # Calculate the mean error per epoch across all bootstrap iterations\n",
    "    mean_train_errors = np.mean(train_errors, axis=0)\n",
    "    mean_val_errors = np.mean(val_errors, axis=0)\n",
    "\n",
    "    # Epochs x-axis\n",
    "    epochs = np.arange(1, len(mean_train_errors) + 1)\n",
    "    \n",
    "   # Plot the learning curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, mean_train_errors, label='Average Train Error', marker='o')\n",
    "    plt.plot(epochs, mean_val_errors, label='Average Validation Error', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(f'{poi} Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_directory = os.path.join(savepath, \"learning_curves\")\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    plot_filename = os.path.join(save_directory, f\"{poi.replace('/', '_')}_learning_curve.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    \n",
    "    plt.clf()  # Clear the figure to free memory\n",
    "    \n",
    "    # Return the minimum average training error\n",
    "    best_train_error = np.min(mean_train_errors)\n",
    "    best_train_rmse = np.sqrt(best_train_error)\n",
    "    best_val_error = np.min(mean_val_errors)\n",
    "    best_val_rmse = np.sqrt(best_val_error)\n",
    "    return best_train_rmse, best_val_rmse\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4326b-1a0a-47e5-be01-6d44e2b005fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through proteins\n",
    "outputdir = \".path_to_output_directory/motor_prot_pred\"\n",
    "for i in range(len(poi_list)):\n",
    "    print(\"i index:\", i)\n",
    "    start_time = time.time()\n",
    "    poi = poi_list[i]\n",
    "    bootstrap = 10\n",
    "    \n",
    "    poi, X, y = data_org(poi, df_reprs_final, prot)\n",
    "\n",
    "    predictions, train_error_dict, valid_error_dict =  bootstrap_nn(poi, X, y, bootstrap)\n",
    "    poi, scorrelation, spvalue, y_avg_pred = plot_correlation(poi, y, predictions, outputdir)\n",
    "    best_train_rmse_error, best_val_rmse_error = plot_learning_curve(poi, train_error_dict, valid_error_dict, outputdir)\n",
    "\n",
    "    predvalues_df = populate_dataframe(predvalues_df, poi, y_avg_pred)\n",
    "    predvalues_df.to_csv(outputdir+'/predvalues.csv', index=False)\n",
    "\n",
    "    df.loc[i] = [poi, scorrelation, spvalue, best_train_rmse_error, best_val_rmse_error]\n",
    "    df.to_csv(outputdir+'/output.csv', index=False)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time spent on grid search: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edbea1-7037-4d6f-b8f6-c28aa8a91e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust p-values\n",
    "def benjamini_hochberg(p_values, alpha=0.05):\n",
    "    m = len(p_values)\n",
    "    sorted_indices = p_values.argsort()\n",
    "    sorted_p_values = p_values[sorted_indices]\n",
    "\n",
    "    adjusted_p_values = sorted_p_values.copy()\n",
    "    for i in reversed(range(m)):\n",
    "        if i == m - 1:\n",
    "            adjusted_p_values[i] = min(sorted_p_values[i], 1)\n",
    "        else:\n",
    "            adjusted_p_values[i] = min(sorted_p_values[i] * m / (i + 1), adjusted_p_values[i + 1])\n",
    "\n",
    "    # Reordering to the original order of p-values\n",
    "    back_order_indices = sorted_indices.argsort()\n",
    "    return adjusted_p_values[back_order_indices]\n",
    "\n",
    "# Calculate BH corrected p-values for spearman_pvalues\n",
    "output['bh_corrected_spearman_pvalue'] = benjamini_hochberg(output['spearman_pvalue'].values)\n",
    "\n",
    "output.to_csv(outputdir+'/output_adjusted.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b61f15-1f30-403d-85b6-b9558d7c9b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sample ID to predvalues csv (optional)\n",
    "def data_org(protein_name, latentdf, prot):\n",
    "\n",
    "     #protein of interest\n",
    "    poi = protein_name #change this to select different proteins\n",
    "    poiname = poi + '_protein'\n",
    "    print(poiname)\n",
    "    #subset protein expression for poi\n",
    "    poidf = prot[[poiname, 'sample_ID']]\n",
    "    #merge dfs and clean for xgboost by removing sample_ID and extraneous columns\n",
    "    merged_df = pd.merge(latentdf, poidf, on='sample_ID')\n",
    "    # Create x and y dataframes\n",
    "    X, y = merged_df.drop(poiname, axis=1), merged_df[[poiname, 'sample_ID']]\n",
    "    return poi, X, y\n",
    "\n",
    "\n",
    "# To get the sample indexes in the correct order as the predicted values dataframe\n",
    "poi, X, y = data_org('ROR1', df_reprs_final, prot)\n",
    "\n",
    "predvalues_df.insert(1, 'sample_ID', y['sample_ID'])\n",
    "predvalues_df.to_csv(outputdir+ '/predvalues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754bb47f-e9b2-46c5-a281-365b82e4b8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7af1c1-f24f-4641-a417-78c52b46c4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
